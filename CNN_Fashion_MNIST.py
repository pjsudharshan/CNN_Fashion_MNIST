# -*- coding: utf-8 -*-
"""CNN_Fashion_MNIST_Sudharshan_PJ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkSYyRmjnrOecnUpjP7m9d8khmVoSlUa

# CNN-based Fashion MNIST classifier
By **Sudharshan P J**
"""

'''
HEADERS
'''

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
from torch.utils.data import DataLoader, RandomSampler
import torch.optim as optim
from torchvision import datasets, transforms
import torch.optim.lr_scheduler as lr_scheduler
import numpy as np
import os
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from torchsummary import summary
import pandas as pd

'''
DEFINE PATH & VARIABLES
'''

# Change below constants if needed

SMALL_SIZE = 8
MEDIUM_SIZE = 10
BIGGER_SIZE = 13

RANDOM_SEED = 12345

DATASET_ROOT = './'
SAVED_MODEL_DIR = './saved_models'
SAVED_IMAGE_DIR = './saved_imgs'
SAVED_SUMMARY_DIR = './saved_summary'
DATASET_MEAN = (0.5)
DATASET_STD = (0.5)
BATCH_SIZE = 100
NUM_EPOCHS = 10
NUM_WORKERS = 2
LEARNING_RATE = 0.001
SAVE_MODEL = True
DROPOUT_PVALS = [0.0, 0.5, 0.7, 0.9, 0.99, 0.999]

# Creates directory if does not exist
if not os.path.exists(SAVED_MODEL_DIR):
    os.makedirs(SAVED_MODEL_DIR)

if not os.path.exists(SAVED_SUMMARY_DIR):
    os.makedirs(SAVED_SUMMARY_DIR)

if not os.path.exists(SAVED_IMAGE_DIR):
    os.makedirs(SAVED_IMAGE_DIR)

# Random seed set
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

# Plot configs
plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=BIGGER_SIZE)     # fontsize of the x and y labels
plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)   # fontsize of the figure title

'''
MODEL
'''

class CNNModel(nn.Module):
    def __init__(self, dropout_p=0.5):
        """
        Initializes a Convolutional based classication network for Fashion MNIST.

        ...

        Attributes
        ----------
        dropout_p : float, optional (default: ``0.5``)
            Dropout probability for the network layers
        """

        super(CNNModel, self).__init__()
        # set up class attributes useful in building the network and inference

        # Convolution Layer 1
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)
        self.relu1 = nn.ReLU()

        # Convolution Layer 2
        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)
        self.relu2 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)

        # Fully connected Layer 1
        self.fc1 = nn.Linear(64*12*12, 128)
        self.relu3 = nn.ReLU()
        self.dropout1 = nn.Dropout(p=dropout_p)

        # Fully connected Layer 2 (readout)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        """
        Forward prop data through the network and return the preds.

        Parameters
        ----------
            x : Tensor of dimension (b, h, w)
                Batch of images of size - (h,w)

        Returns
        -------
            out : Tensor of dimension (b, num_classes)
                predictions (logits) of the model
        """

        # Convolution 1
        out = self.cnn1(x)
        out = self.relu1(out)

        # Convolution 2 
        out = self.cnn2(out)
        out = self.relu2(out)
        out = self.maxpool1(out)

        # Resize for FC1
        out = out.view(out.size(0), -1)

        # Fully connected Layer 1
        out = self.fc1(out)
        out = self.relu3(out)
        out = self.dropout1(out)

        # Fully connected Layer 2 (readout)
        out = self.fc2(out)

        return out

'''
MODEL PARAMETERS CHECK
'''

def check_model(model):
    """
    Prints a string summarizing the network

    Parameters
    ----------
        model: Class definition
            Class where the neural net is initialized

    Returns
    -------
    None
    """

    summary(model, (1, 28, 28))

'''
CHECKED
'''

check_model(CNNModel().cuda())

'''
MODEL TRAINER
'''

def train_CNNmodel(train_loader, test_loader, num_epochs=10, learning_rate=0.001, 
                   dropout_p=0.5, use_scheduler=False, save_model=False):
    """
    Trains the CNN model for the classification task using Adam optimizer and Cross Entropy Loss.
    Saves the model after ``num_epochs`` epochs along with the accuracy and loss statistics.

    Displays a nice training progress bar with epoch statistics.
    See `https://tqdm.github.io/docs/notebook/` documentation page for more details.

    Parameters
    ----------
        train_loader : Class DataLoader
             Provides an iterable over the train dataset
        test_loader : Class DataLoader
             Provides an iterable over the test dataset
        num_epochs : int, optional (default: ``10``)
            Number of epochs to run
        learning_rate : float, optional (default: ``0.001``)
            learning rate for the optimizer
        dropout_p : float, optional (default: ``0.5``)
            Dropout probability for the network layers
        use_scheduler : bool, optional (default: ``False``)
            If ``True``, learning rate scheduler is used
        save_model : bool, optinal (default: ``False``)
            Set to ``True`` if the model has to be saved

    Returns
    -------
    None
    """
    
    train_nepochs_loss = []
    test_nepochs_loss = []
    test_nepochs_accuracy = []

    # INSTANTIATE MODEL INSTANCE
    model = CNNModel(dropout_p=dropout_p).cuda()

    # INSTANTIATE LOSS INSTANCE
    criterion = nn.CrossEntropyLoss()

    # INSTANTIATE OPTIMIZER CLASS
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    if use_scheduler:
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader))

    print('\nSETTINGS')
    print(f'TOTAL #EPOCHS: {num_epochs}; LEARNING RATE: {learning_rate}; DROPOUT PROBABILITY: {dropout_p}')

    for epoch in range(1, num_epochs+1):
        train_loss = []
        test_loss = []

        model.train()
        with tqdm(total=len(train_loader.dataset), desc=f'Epoch {epoch}/{num_epochs}', 
                unit='images', position=0, leave=True) as pbar:

            for batch_idx, (images, labels) in enumerate(train_loader):
                images = images.cuda()
                labels = labels.cuda()
                
                # Clear gradients w.r.t. parameters
                optimizer.zero_grad()

                # Forward pass to get output/logits
                outputs = model(images)

                # Calculate Loss: softmax --> cross entropy loss
                loss = criterion(outputs, labels)
                pbar.set_postfix(**{'train loss (batch)': loss.item()})

                # Getting gradients w.r.t. parameters
                loss.backward()

                # Updating parameters
                optimizer.step()
                
                train_loss.append(loss.item())
                pbar.update(images.shape[0])

            # Learning rate scheduler
            if use_scheduler:
                scheduler.step()

            train_loss_mean = np.mean(train_loss)

        # Iterate through test dataset
        model.eval()
        with torch.no_grad():  
            num_correct = 0
            num_total = 0

            for images, labels in test_loader:
                images = images.cuda()
                labels = labels.cuda()
                
                # Forward pass only to get logits/output
                outputs = model(images)
                
                # Get predictions from the maximum value
                _, predicted = torch.max(outputs.data, 1)

                loss = criterion(outputs, labels)
                test_loss.append(loss.item())

                # Total number of labels
                num_total += labels.size(0)
                
                num_correct += (predicted == labels).sum()

            test_loss_mean = np.mean(test_loss)
            test_accuracy = (num_correct.item()/num_total)*100
            
        # Print Loss
        epoch_output = '[EPOCH: {}/{}] Train Loss: {:.4f}; Test Loss: {:.4f}; Test Accuracy: {:.4f} %'
        print(epoch_output.format(epoch, num_epochs, train_loss_mean, test_loss_mean, test_accuracy))
        train_nepochs_loss.append(train_loss_mean)
        test_nepochs_loss.append(test_loss_mean)
        test_nepochs_accuracy.append(test_accuracy)
    
    print(80*'#') # End marker

    stat_summary = pd.DataFrame({'train_loss':train_nepochs_loss, 
                        'test_loss':test_nepochs_loss, 
                        'test_accuracy':test_nepochs_accuracy})
    
    summary_filename = f'ne{num_epochs}lr{learning_rate}dp{dropout_p}'
    stat_summary.to_csv(os.path.join(SAVED_SUMMARY_DIR, f'{summary_filename}.csv'), index=False)
    
    # Save model
    if SAVE_MODEL:
        model_filename = f'{summary_filename}.pth'
        torch.save(model.state_dict(), os.path.join(SAVED_MODEL_DIR, model_filename))

'''
MAIN RUNNER
'''

if __name__ == "__main__":
    """
    Train and Test Transforms are defined along with their respective dataloader.
    Runs the function `train_CNNmodel` for different dropout probabilities
    """

    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(), # RandomHorizontalFlip Data Augmentation
        transforms.ToTensor(), 
        transforms.Normalize(DATASET_MEAN, DATASET_STD)
    ])
    
    test_transform = transforms.Compose([ 
        transforms.ToTensor(), 
        transforms.Normalize(DATASET_MEAN, DATASET_STD)
    ])

    train_set = datasets.FashionMNIST(root=DATASET_ROOT, train=True, 
                                      transform=train_transform, download=True)

    test_set = datasets.FashionMNIST(root=DATASET_ROOT, train=False, 
                                     transform=test_transform)

    train_loader = DataLoader(dataset=train_set, 
                              batch_size=BATCH_SIZE, 
                              shuffle=True, 
                              num_workers=NUM_WORKERS)

    test_loader = DataLoader(dataset=test_set, 
                             batch_size=BATCH_SIZE, 
                             shuffle=False, 
                             num_workers=NUM_WORKERS)

    for pvals in DROPOUT_PVALS:
        train_CNNmodel(train_loader, test_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
                       dropout_p=pvals, use_scheduler=False, save_model=SAVE_MODEL)

"""* We have saved the trained model after 10th epoch along with its accuracy & loss statistics.
  * This saved model can later be used for deployment or further training starting from 10th epoch onwards
  * The obtained accuracy and loss statistics can be used to visualize the model performance at a later stage
"""

'''
STATS PLOTTING
'''

def plot_test_eval(num_epochs, learning_rate, dropout_pvals, loss_or_accuracy='accuracy'):
    """
    Loads the loss or accuracy statistics and plots the curve - (test accuracy vs epochs) 
    or (test loss vs epochs) for different dropouts.

    Parameters
    ----------
        num_epochs : int
            Number of epochs to run
        learning_rate : float
            learning rate for the optimizer
        dropout_pvals : list
            List of Dropout probabilies (float)
        loss_or_accuracy : str, optional (default: ``accuracy``)
            Choose any of [``accuracy``,``loss``]

    Returns
    -------
    None
    """

    plt.figure(figsize=(12, 8))
    x_ticks = np.arange(1,num_epochs+1)

    for pvals in dropout_pvals:
        stat_summary = pd.read_csv(os.path.join(SAVED_SUMMARY_DIR, 
                                                f'ne{num_epochs}lr{learning_rate}dp{pvals}.csv'))
        plt.plot(x_ticks, stat_summary[f'test_{loss_or_accuracy}'], marker='d', label=f'p={pvals}')

    plt.legend(title='Dropout', loc='best')
    plt.title('Test Set Evaluation')
    plt.xlabel('Epochs')
    percent_sym = '(%)' if loss_or_accuracy=='accuracy' else ''
    plt.ylabel(f'{loss_or_accuracy} {percent_sym}')
    plt.savefig(os.path.join(SAVED_IMAGE_DIR, f'test_{loss_or_accuracy}.svg'))
    plt.xticks(x_ticks)
    plt.grid()
    plt.show()

'''
PLOTTING TEST LOSS
'''

plot_test_eval(num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
                    dropout_pvals=DROPOUT_PVALS, loss_or_accuracy='loss')

'''
PLOTTING TEST ACCURACY
'''

plot_test_eval(num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, 
                    dropout_pvals=DROPOUT_PVALS, loss_or_accuracy='accuracy')

"""### From above, we can clearly see that the best model is the one with droput probability = 0.0"""

'''
IMAGE, PREDICTED AND TRUE LABELS
'''

def load_and_display(num_epochs=10, learning_rate=0.001, dropout_p=0.0):
    """
    Loads the saved model, invokes the dataloader, randomly evaluates on 5 samples 
    of the test set and displys the images with the true and predicted labels.

    Parameters
    ----------
        num_epochs : int, optional (default: ``10``)
            Number of epochs to run
        learning_rate : float, optional (default: ``0.001``)
            learning rate for the optimizer
        dropout_p : float, optional (default: ``0.0``)
            Dropout probability for the network layers

    Returns
    -------
    None
    """

    model_filename = f'ne{num_epochs}lr{learning_rate}dp{dropout_p}'
    model = CNNModel().cuda()
    model.load_state_dict(torch.load(os.path.join(SAVED_MODEL_DIR, f'{model_filename}.pth')))
    model.eval()

    test_set = datasets.FashionMNIST(root=DATASET_ROOT, train=False, 
                                     transform=transforms.ToTensor())

    sampler = RandomSampler(data_source=test_set)
    test_loader = DataLoader(dataset=test_set, batch_size=5, sampler=sampler)


    images, labels = next(test_loader.__iter__())
    images = images.cuda()
    labels = labels.cuda()

    logits = model(images)
    probs = F.softmax(logits.data, dim=1)
    _, predicted = torch.max(probs, 1)

    # Invert the dictionary class_to_idx to idx_to_class
    idx_to_class = {v: k for k, v in test_set.class_to_idx.items()}

    fig, axex = plt.subplots(1, 5, figsize=(25,25))

    zip_gen = axex.ravel(), predicted, labels, images.cpu().numpy().squeeze()
    for ax, predicted_class, label_class, img in zip(*zip_gen):
        ax.imshow(img, cmap='gray' if predicted_class == label_class else 'autumn')
        ax.axis('off')
        ax.set_title('Predicted: {} | True: {}'.format(idx_to_class[predicted_class.item()], 
                                     idx_to_class[label_class.item()]))

'''
PREDICTION OF 5 RANDOMLY SAMPLED IMAGES
'''

load_and_display(num_epochs=10, learning_rate=0.001, dropout_p=0.0)